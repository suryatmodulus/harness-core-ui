createNewConnector: Create New Connector
confirmDeleteTitle: Delete Connector
editConnector: Edit the Connector
confirmDelete: Are you sure you want to delete the Connector
testInProgress: Test in progress
artifactRepository: ' Artifact {{ $.repository }}'
artifactRepoType: 'Artifact {{$.repositoryType}}'
specifyArtifactRepoType: 'Specify Artifact {{ $.repositoryType }}'
specifyArtifactRepo: 'Specify Artifact {{ $.repository }}'
newArtifactRepository: 'New Artifact Repository Connector'
selectConnectorLabel: Select Artifact Repository Connector
stepThreeName: '{{$.common.labelTestConnection}}'
stepFourName: 'Artifact Location'
name: Connector Name
updating: 'Updating Connector {{name}}'
creating: 'Creating Connector {{name}}'
successfullCreate: 'Connector {{name}} created successfully'
successfullUpdate: 'Connector {{name}} updated successfully'
connectorNotFound: Connector not found.
scopeError: 'Cannot create a Connector at {{createdAtScope}} scope from {{createdFromScope}} scope.'
createFromYaml: 'Create New Connector from YAML'
selectConnectivityMode: 'Select Connectivity Mode'
successfullyCreated: 'Connector created successfully'
secretManagerDetails: 'Secret Manager Details'
gcrConnectorDetails: 'Google Container Registry Overview'
hashicorpVaultDetails: 'HashiCorp Vault Overview'
appDynamicsDetails: 'AppDynamics Connector Details'
splunkConnectorDetails: 'Splunk Connector Details'
datadogConnectorDetails: 'Datadog Connector Details'
pagerDutyConnectorDetails: 'PagerDuty Connector Details'
sumoLogicConnectorDetails: 'SumoLogic Connector Details'
newRelicConnectorDetails: New Relic Connector Details
prometheusConnectorDetails: Prometheus Connector Details
customConnectorDetails: Custom Health Connector Details
prometheusLabel: Prometheus
dynatraceLabel: Dynatrace
appdLabel: AppDynamics
newRelicLabel: New Relic
splunkLabel: Splunk
customLabel: Custom Health
stackdriverMetricsLabel: Google Cloud Operations (Metrics)
stackdriverLogsLabel: Google Cloud Operations (Logs)
click: 'Click'
dynatraceConnectorDetails: Dynatrace Connector Details
connectorDetailsHeader: '{{type}} Connector Details'
addConnectorDetails: Add {{type}} Verification Provider
unableToCreateConnector: Unable to create Connector
unableToUpdateConnector: Unable to update Connector
encryptedAPIKeyLabel: Encrypted API Key
encryptedAPIKeyValidation: Encrypted API Key is required.
tenantId: Tenant Id
tenantIdRequired: Tenant Id is required
reportNameRequired: '{{ $.common.reportNameRequired }}'
connectorAlreadyExist: Connector already exists for the cloud account
loginToMasterAccount: and login to your master account
roleARN: Create Cross Account Role
ifReq: if required.
selectConnector: Select Connector
showInstructions: Show instructions
apiKey: '{{$.common.apikey}}'
apiKeyOrPassword: Password/Api Key
costVisibility: Cost Visibility
chooseMethodForK8sConnection: Choose the method for Harness to use when connecting to the cluster.
chooseMethodForGCPConnection: Choose the method for Harness to use when connecting to GCP.
delegateInClusterInfo: 'Use the credentials of a specific Harness Delegate (IAM role, service account, etc)'
createConnector: 'Create a Connector'
connectorEmptyState: 'There are no connectors in your project'
readMore: 'Read more'
connectorsTitle: 'Resources: Connectors'
parameters: Parameters
validationPath: Validation Path
addHeader: Add Header
addParameter: Add Parameter
baseURL: Base URL
requestMethod: Request Method
connectivityMode:
  title: 'Connectivity Mode'
  validation: 'Please select connectivity mode'
  connectToProvider: 'Connect to the provider'
  selectText: 'Select how you would like to connect to the provider'
title:
  k8sCluster: 'Kubernetes Cluster'
  gcpConnector: 'Google Cloud Provider'
  helmConnector: 'HTTP Helm Repo'
  gitConnector: 'Git Connector'
  githubConnector: 'GitHub Connector'
  gitlabConnector: 'GitLab Connector'
  bitbucketConnector: 'Bitbucket Connector'
  hashicorpVault: 'HashiCorp Vault'
  jira: 'Jira'
  serviceNow: 'ServiceNow'
  secretManager: 'Secret Manager'
  appdynamics: 'AppDynamics Connector'
  splunk: 'Splunk Connector'
  aws: 'AWS Cloud Provider'
  awsCodeCommit: 'AWS CodeCommit'
  nexus: 'Nexus Repository'
  artifactory: 'Artifactory Repository'
  awsKms: 'AWS Key Management Service'
  awsSecretManager: 'AWS Secrets Manager'
  delegateSelection: 'Delegate Selection'
  ceAzureConnector: 'Azure Connector'
  datadog: 'Datadog'
  azureKeyVault: 'Azure Key Vault'
  sumologic: 'SumoLogic'
  ceAws: 'AWS Connector '
  gcpKms: GCP Key Management Service
  errorTracking: 'Error Tracking'
name_labels:
  Kubernetes: 'Kubernetes Connector Name'
  HttpHelmRepo: 'Helm Http Connector Name'
  Git: 'Git Connector Name'
  Github: 'GitHub Connector Name'
  Gitlab: 'GitLab Connector Name'
  Bitbucket: 'Bitbucket Connector Name'
  Docker: 'Docker Connector Name'
  GCP: 'GCP Connector Name'
  gcpKms: 'GCP KMS Connector Name'
  AWS: 'AWS Connector Name'
  AwsCodeCommit: 'AWS CodeCommit Name'
  ECR: ECR Connector Name'
  Nexus: 'Nexus Connector Name'
  Artifactory: 'Artifactory Connector Name'
  SecretManager: 'Secret Manager Name'
  AppDynamics: 'AppDynamics Connector Name'
  Splunk: 'Splunk Connector Name'
testConnectionStep:
  errorDetails: Click to view error details
  noDelegate: Unable to get Delegate information
  executingOn: 'Executed on : '
  verificationSuccessful: Verification successful
  viewPermissions: View permissions required
  installNewDelegate: Install new Delegate
  placeholderError: Test failed for the Connector
  url:
    k8s: 'Master URL : '
    docker: 'Docker Registry URL : '
    artifactory: 'Artifactory Repository URL : '
    nexus: 'Nexus Repository URL : '
    splunk: 'Splunk URL : '
    appD: 'Controller URL : '
    vault: 'Vault URL : '
    bitbucket: 'URL : '
    gcr: 'GCR URL : '
  validationText:
    k8s: Validating the reachability of provided URL
    docker: Validating Docker Registry authentication and permissions
    artifactory: Validating Artifactory Repository authentication and permissions
    nexus: Validating Nexus Repository authentication and permissions
    gcp: Validating Google Cloud Provider authentication and permissions
    gcr: Validating Google Container Registry authentication and permissions
    aws: Validating the AWS Cloud Provider authentication and permissions
    appD: Validating AppDynamics authentication and permissions
    splunk: Validating Splunk authentication and permissions
    vault: Validating HashiCorp Vault authentication and permissions
    awsSecretManager: Validating AWS Secret Manager authentication and permissions
    bitbucket: Validating Bitbucket authentication and permissions
    gitlab: Validating GitLab authentication and permissions
    github: Validating GitHub authentication and permissions
    git: Validating Git authentication and permissions
    azure: Validating Azure authentication and permissions
    datadog: Validating Datadog authentication and permissions
    pagerduty: Validating PagerDuty authentication and permissions
    azureKeyVault: Validating Azure key vault authentication and permissions
    jira: Validating Jira authentication and permissions
    serviceNow: Validating ServiceNow authentication and permissions
    sumologic: Validating Sumologic authentication and permissions
    gcpKms: Validating GCP Key Management Service authentication and permissions
validation:
  serviceNowUrl: 'ServiceNow URL is required.'
httpHelm:
  httpHelmRepoUrl: 'Helm Repo URL'
jira:
  jiraUrl: 'Jira URL'
serviceNow:
  serviceNowUrl: 'ServiceNow URL'
k8:
  delegateOutClusterInfo: 'Specify master URL and credentials'
  delegateInClusterInfo: '{{ $.connectors.delegateInClusterInfo }}'
  masterUrlLabel: 'Master URL'
  serviceAccountToken: 'Service Account Token'
  serviceAccountKey: 'Service Account Key'
  skipDefaultValidation: 'Skip default namespace validation'
  OIDCIssuerUrl: OIDC Issuer URL
  OIDCUsername: 'OIDC Username'
  OIDCPassword: 'OIDC Password'
  OIDCClientId: 'OIDC Client Id'
  OIDCSecret: 'OIDC Secret'
  clientSecretOptional: 'Client Secret (optional)'
  OIDCScopes: 'OIDC Scopes (optional)'
  clientKey: 'Client Key'
  clientKeyPassphrase: 'Client Key Passphrase (optional)'
  clientCertificate: 'Client Certificate'
  clientKeyAlgorithm: 'Client Key Algorithm'
  clientKeyAlgorithmPlaceholder: Select or Enter a Client Key Algorithm
  clientKeyCACertificate: 'CA Certificate (optional)'
  placeholder:
    masterUrl: '/URL'
  authLabels:
    OIDC: 'OpenID Connect'
    clientKeyCertificate: 'Client Key Certificate'
  validation:
    clientKeyAlgo: Client Key Algorithm is required
hashiCorpVault:
  appRole: App Role
  appRoleId: 'App Role Id'
  baseSecretPath: 'Base Secret Path (optional)'
  secretId: 'Secret Id'
  stepTwoName: 'HashiCorp Vault Details'
  vaultUrl: 'Vault URL'
  secretEngine: Secret Engine
  engineName: 'Secret Engine Name'
  engineVersion: 'Secret Engine Version'
  renewal: 'Renewal Interval (minutes)'
  readOnly: '{{$.common.readOnly}}'
  readOnlyVault: 'Read-only Vault'
  default: '{{$.common.default}}'
  defaultVault: 'Use as Default Secrets Manager'
  fetchEngines: Fetch Engines
  setupEngine: Setup Engine
  saveInProgress: Saving Connector Details
  manuallyConfigureEngine: Manually Configure Engine
  vaultAgent: Vault Agent
  root: '/(Root)'
  sinkPath: Sink Path
  sinkPathIsRequired: Sink Path is required
nexus:
  nexusLabel: Nexus
  nexusServerUrl: 'Nexus Repository URL'
artifactory:
  artifactoryLabel: Artifactory
  artifactoryServerUrl: 'Artifactory Repository URL'
GCP:
  delegateOutClusterInfo: 'Specify credentials here'
  delegateInClusterInfo: '{{ $.connectors.delegateInClusterInfo }}'
ECR:
  name: ECR
  fullName: ECR Container Registry
GCR:
  name: GCR
  fullName: Google Container Registry
  artifactServer: 'GCR Server'
  stepTwoName: 'Google Container Registry Details'
  registryHostname: 'GCR Registry URL'
GCS:
  name: GCS
  fullName: Google Cloud Storage
S3: S3
docker:
  dockerRegistryURL: 'Docker Registry URL'
  dockerProvideType: 'Provider Type'
  dockerHub: 'DockerHub'
  harbour: 'Harbour'
  quay: 'Quay'
  other: 'Other (Docker V2 compliant)'
  dockerRepository: Docker Repository
helmRepo:
  helmRepoUrl: 'Helm Repository URL'
aws:
  awsAccessKey: AWS Access Key
  accessKey: '{{ $.common.accessKey }}'
  secretKey: '{{ $.common.secretKey }}'
  enableCrossAcc: Enable cross-account access (STS Role)
  crossAccURN: Cross account role ARN
  externalId: External Id (Optional)
  assumeIAMRole: Assume IAM role on Delegate
  useIRSA: Use IRSA
  validation:
    delegateSelector: 'Delegate Tag is required'
    secretKeyRef: 'Secret key is required'
    accessKey: 'Access key is required'
    crossAccountRoleArn: 'Role ARN is required'
awsCodeCommit:
  repoUrl: 'AWS CodeCommit Repository URL'
appD:
  connectionDetailsHeader: AppDynamics Connection Details
  apiClient: API Client
  clientId: '{{ $.common.clientId }}'
  controllerURL: Controller URL
  accountName: '{{ $.common.accountName }}'
  validation:
    controllerURL: Controller URL is a required field
    clientId: Client Id is a required field
    clientSecret: Client Secret is a required field
newRelic:
  subTitle: To add New Relic Connector, you will need access to New Relic Insights API. Verification analysis is limited to APM only.
  urlFieldLabel: New Relic URL
  accountIdFieldLabel: New Relic Account Id
  urlValidation: New Relic URL is required.
  accountIdValidation: New Relic Account Id is required.
  accountIdTooltip: For steps to get New Relic Account Id
  products:
    fullStackObservability: 'Full Stack Observability: APM'
prometheus:
  urlValidation: Prometheus URL is required.
datadog:
  encryptedAPPKeyLabel: Encrypted APP Key
  urlValidation: Datadog URL is required.
  encryptedAPPKeyValidation: Encrypted APP Key is required.
errorTracking:
  urlValidation: Error Tracking URL is required.
  sidValidation: Error Tracking SID is required.
  sidLabel: SID
dynatrace:
  apiToken: API Token
  apiTokenValidation: API Token is required.
  urlValidation: Dynatrace URL is required.
sumologic:
  urlLabel: Sumo Logic API Server URL
  encryptedAccessIdLabel: Encrypted Access Id
  encryptedAccessKeyLabel: Encrypted Access Key
  urlValidation: SumoLogic URL is required.
  encryptedAccessIdValidation: SumoLogic Access Id is required.
  encryptedAccessKeyValidation: SumoLogic Access Key is required.
customHealth:
  baseURL: Base URL is required.
  requestBody: Request Body is required for POST requests.
  requestMethod: Request Method is required.
  validationPath: Validation Path is required.
  valueRequired: Value is required.
  keyRequired: Key is required.
cdng:
  jobName: Job Name
  continousVerificationType: Continous Verification Type
  artifactTag: Artifact Tag
  defineVerificationJob: Define Verification Job
  baseline: Baseline
  trafficsplit: Traffic Split (OPTIONAL)
  continousVerificationStep: Verify Step Details
  displayName: Display Name
  createCVJob: Please create a verification job
  loadingJobs: Loading verification jobs…
  noJobsConfigured: No verification jobs are configured.
  selectTheJobNameFirst: Please select the job name first by clicking on Define Verification Job Panel
  error: Something went wrong , please try again by clicking on CV step again
  monitoredService:
    label: Monitored Service
    fetchingMonitoredService: Fetching Monitored service ...
    creatingMonitoredService: Creating Monitored service ...
    fetchingMonitoredServiceError: Failed to fetch monitored service . Please try again by reopening the Verify Step
    autoCreateMonitoredService: Click to autocreate a monitored service
    creatingMonitoredServiceError: Failed to create monitored service . Please try again by reopening the Verify step and selecting service, environment.
    monitoredServiceText: A monitored service for the
    backToVerifyStep: Back to Verify Step
  runTimeMonitoredService:
    pleaseSpecify: Please specify
    toFetchMonitoredService: to fetch the monitored service.
    fetchingMonitoredServiceError: Failed to fetch monitored service . Please try again by re-running the Pipeline
    noMonitoringSercvicePresent: No Monitored Service is present for selected service and environment. Verify step will be skipped.
    noHealthSourcePresent: No Health source is present . Verify step will be skipped.
    backToRunPipeline: Back to Run Pipeline
  healthSources:
    label: Health Sources
    noHealthSourcesDefined: No health sources have been defined. To complete the verification, you would need at least one healthsource added.
  validations:
    deploymentTagRequired: Deployment Tag is required
    jobNameRequired: Job name is required
    durationRequired: Duration is required
    sensitivityRequired: Sensitivity is required
    verificationTypeRequired: Verification type is required
    monitoringServiceRequired: Monitored Service is required
    healthSourceRequired: At least one health source is required
  verificationSensitivityLabel:
    high: High
    medium: Medium
    low: Low
  baselineDefaultLabel:
    lastSuccess: Last Successful job run
  jobTypes:
    test: TEST
    blueGreen: BLUE_GREEN
    canary: CANARY
    health: HEALTH
connectAndSave: Connect and Save
connectorDetails: Connector Details
verifyConnection: Verify connection
createdSuccessfully: 'Connector {{name}} created successfully'
updatedSuccessfully: 'Connector {{name}} updated successfully'
splunk:
  connectorDetailsHeader: Splunk Connection Details
awsSecretManager:
  secretNamePrefix: Secret Name Prefix
gcpKms:
  keyRing: Key Ring
  keyName: Key Name
  credentialsFile: GCP KMS Credentials File
  keyRingRequired: Key Ring is required
  keyNameRequired: Key Name is required
  credentialsFileRequired: Credentials file is required
awsKms:
  accessKeyLabel: 'AWS - Access Key Id'
  secretKeyLabel: 'AWS - Secret Access Key'
  arnLabel: 'AWS ARN'
  roleArnLabel: 'Role ARN'
  assumedRoleDuration: 'Assumed Role Duration (seconds)'
  loggedAt: 'Logged at'
  validation:
    selectDelegate: 'Please select a Delegate'
    selectRegion: 'Please select a Region'
    selectAWSArn: 'AWS ARN is required'
    durationNumber: 'Duration must be between 900 and 43200 seconds'
    durationError: 'Duration must be a number'
    externalIdRegexError: 'External Id can have upper and lower case alphanumeric characters with no spaces. You can also include underscores or any of the following characters: =,.@:/-'
    externalIdLengthError: 'External Id must have between 2 and 1224 characters'
  awsSTS: 'Assume Role using STS on Delegate'
delegate:
  configure: Configure what Delegates are allowed to connect to this Connector
  delegateSelectorAny: Connect via any available Delegate
  delegateSelectorSelective: Connect only via Delegates which has all of the following tags
  hearbeat: heartbeat
  matchesSelectors: Matches
  testDelegateConnectivity: Test Delegate Connectivity
  matchingDelegates: matching Delegates
  waitingForConnection: Waiting for connection
  noDelegates: You have no Delegates
  noMatchingDelegate: No eligible Delegates found. Install a new Delegate or continue to save.
  delegateselectionPlaceholder: Select or Enter Delegate Selectors
  noMatchingDelegatesActive: None of the matching Delegates are in active state.
  couldNotFetch: Failed to update the Delegate list, Will try again in {{pollingInterval}}
  delegateSelectors: Delegate Selectors
ceK8:
  infoText: 'If you have previously created a connector to the preferred cluster for any of the other modules, you may reference the same connector here and create a connector for Cloud cost management. If this is the first time creating a connector to this cluster, you may proceed without referencing.'
  selectConnectorLabel: 'Reference an existing CD Kubernetes connector'
  overview:
    createNewConnectorCta: 'Create a new connector'
  chooseRequirements:
    heading: '{{ $.connectors.ceAzure.steps.requirements }}'
    subheading: '{{ $.connectors.ceAzure.chooseRequirements.featureDesc }}'
    description: '(Deep Kubernetes cost Visibility is selected by default. Select the other option to enable Intelligent Cloud AutoStopping for Kubernetes)'
    fixFeaturesDescription: 'Both the options below are required for Kubernetes AutoStopping and are selected by default. Click Continue to proceed.'
    visibility:
      heading: 'DEEP KUBERNETES'
      subheading: '{{ $.connectors.costVisibility }}'
      description: '<ul><li>Costs by pods, nodes, namespaces, workloads </li><li> Idle and unallocated cluster costs</li><li> Cost anomaly detection</li><li> Workload recommendations</li><li> Node recommendations</li><li> Budgets and alerts</li></ul>'
    optimization:
      heading: 'KUBERNETES OPTIMIZATION BY'
      subheading: '{{ $.common.ce.autostopping }}'
      description: '<ul><li>Works for custom clusters and EKS, AKS, GKE, etc</li><li> Orchestrate pods based on idleness</li><li> Set dependencies between workloads like pods, VMs, etc.</li><li> Granular savings visibility</li><li> Simple one-time setup</li></ul>'
  secretCreationStep:
    step1: 'Create an API key'
    step2: 'Run the following command to create a namespace'
    step3: 'Replace the API key in the following YAML'
    step4: 'Run the following command to apply the YAML and create a secret'
    namespaceCommand: 'kubectl create namespace harness-autostopping'
    creationCommand: 'kubectl apply -f secret.yaml'
  providePermissionsStep:
    heading: 'Provide Permissions - Download YAML'
    info: 'If you are using a EKS, ensure that the metrics server is installed on your Kubernetes cluster. '
    downloadYamlText: 'To provide required permissions to your cluster, please download the YAML below and continue to apply it using instructions given in the next step. You can preview the YAML file '
    fileDescription:
      heading: 'This YAML file contains:'
      info1: 'Creation of a delegate'
      info2: 'Permissions to access the pods and services of the cluster'
      info3: 'Installing components to create Autostopping rules'
      info4: 'Permissions to start and stop services as per rules'
    downloadYamlBtnText: 'Download YAML'
    downloadComplete: 'Download Complete'
    applyDelegateText: 'Copy the downloaded YAML to a machine where you have kubectl installed and have access to your Kubernetes cluster. Run the following command to apply the Harness delegate to your Kubernetes Cluster'
    successfulCommandExec: 'Command executed successfully'
  featureWarning: You have used {{count}} / {{limit}} free clusters incuded in your current plan. Consider upgrading for unlimited clusters.
azureKeyVault:
  labels:
    tenantId: '{{ $.connectors.tenantId }}'
    subscription: Subscription
    vaultName: Vault Name
    fetchVault: Fetch Vault
    setupVault: Setup Vault
  validation:
    tenantId: '{{ $.connectors.tenantIdRequired }}'
    subscription: Subscription is required
    vaultName: Vault Name is required
ceAws:
  steps:
    overview: '{{ $.overview }}'
    cur: 'Cost and Usage Report'
    req: 'Choose requirements'
    roleARN: '{{ $.connectors.roleARN }}'
    test: '{{ $.common.labelTestConnection }}'
  overview:
    heading: '{{ $.overview }}'
    awsAccountId: 'Specify the AWS account Id'
    alreadyExist: ' {{ $.connectors.connectorAlreadyExist }} '
    alreadyExistInfo: 'The cloud account {{awsAccountID}} already has a Connector “{{existingConnectorName}}” linked to it. The Connector has permission for {{featureText}}'
    trySuggestion: '{{ $.common.errorHandler.tryTheseSuggestions }}'
    editConnector: '{{ $.connectors.editConnector }}'
    ifReq: '{{ $.connectors.ifReq }}'
    validation:
      numeric: 'AWS Account Id must be numeric'
      positive: 'AWS Account Id must be a positive number'
      required: 'AWS Account Id is required'
  cur:
    heading: '{{ $.connectors.ceAws.steps.cur }}'
    subheading: 'Cost and Usage Report provides detailed billing data across AWS accounts to help you analyze your spend.'
    followInstruction: 'Please follow the instructions to provide access to the Cost and Usage Report'
    launchTemplate: 'Launch AWS console'
    login: 'and login to your account'
    createNew: '+ Create new Cost and Usage report'
    reportName: 'Cost and Usage Report Name'
    bucketName: 'Cost and Usage S3 bucket Name'
    validation:
      reportRequired: '{{ $.common.reportNameRequired }}'
      bucketRequired: 'Bucket name is required'
  curExising:
    subHeading: 'The following Cost and Usage Reports exist in your Harness account. If the AWS Account Id you entered is covered by any of the below, then click “Continue” to proceed. If not, click on "Create new Cost and Usage Report.'
    accountID: 'Account Id '
  curExtention:
    heading: 'How to Create Cost and Usage Report?'
    subtext: 'Once logged into the AWS console'
    stepA:
      heading: 'Creating a Report'
      step1:
        p1: 'Create Report'
        p2: 'and provide a Cost and Usage Report Name. Copy this and paste it in the space provided on the left.'
      step2: 'Check Include resource IDs and keep other selections as it is.'
      step3: 'Click Next.'
    stepB:
      heading: 'Delivery options'
      step1:
        p1: 'Configure'
        p2: 'to create an S3 bucket.'
      step2: 'Provide an S3 bucket name and select Region (preferably US East - N.Virginia).'
      step3:
        p1: ', then click '
      step4: 'Enter a Report path prefix.'
      step5:
        heading: 'Configure the report with the following configuration:'
        subStep1:
          p1: 'Time granularity: '
          p2: 'Hourly '
        subStep2:
          p1: 'Report versioning: '
          p2: 'Overwrite Existing Report Version'
        subStep3: 'You do not need to enable any report data integrations.'
        subStep4:
          p1: 'Compression: '
          p2: 'GZIP'
      step6:
        p1: '{{ $.connectors.ceAws.curExtention.stepB.step3.p1 }}'
        p2: 'Review and Complete'
      step7: 'Copy and enter S3 bucket name.'
    moreHelp:
      heading: 'More help:'
      step1: '{{$.connectors.ceAzure.billing.extension.video}}'
      step2: '{{$.connectors.ceAzure.billing.extension.docs}}'
      step3: 'Creating Cost and Usage Reports'
      step4: 'Billing and Cost Management Policy Examples'
  crossAccountRoleStep1:
    heading: 'Create Cross Account Role - '
    choosePermissions: '{{ $.connectors.ceAzure.chooseRequirements.heading }}'
    subHeading:
      'Harness uses the secure cross-account role to access your AWS account. The role includes a restricted policy to
      access the cost and usage reports and resources for the sole purpose of cost analysis and cost optimization'
    description: 'Select the Cloud Cost Management features you would like to use on the AWS account.'
    info: (AWS cost Visibility is available by default. Select the others as per your requirements)
    cost: 'Cost'
    visibility: '{{ $.common.ce.visibility }}'
    optimization: '{{ $.common.ce.optimization }}'
    visibilityDes: '{{ $.connectors.ceAzure.chooseRequirements.visibilityCardDesc }}'
    optimizationDes: '{{ $.connectors.ceAzure.chooseRequirements.optimizationCardDesc }}'
    default:
      feat1: AWS costs by services, accounts, etc.
      footer: from Cost and Usage Report
    visible:
      feat1: EC2, EBS and snapshot inventory dashboards
      feat2: ECS cluster cost, tasks, lauchtype details
      prefix: AWS ECS & RESOURCE
      heading: Inventory management
    optimize:
      feat1: Orchestrate VMs and ASGs based on idleness
      prefix: AWS OPTIMIZATION BY
      footer: to IAM role
  crossAccountRoleStep2:
    heading: ' {{$.connectors.ceAws.crossAccountRoleStep1.heading}} '
    createRole: 'Create Role'
    subHeading: '{{ $.connectors.ceAws.crossAccountRoleStep1.subHeading }}'
    launchTemplate: 'Launch Template In AWS console'
    followInstructions: 'and follow the instuctions given on the right'
    roleArn: '{{ $.connectors.awsKms.roleArnLabel }}'
    extId: 'External Id'
    dontHaveAccess: 'Don’t have access to add an AWS account?'
    validation:
      roleArnPattern: 'Role ARN does not match the pattern'
      roleArnRequired: 'Role ARN is Required'
  crossAccountRoleExtention:
    heading: 'Create cross-account IAM role using AWS CloudFormation template'
    subHeading: 'Launch the template in the AWS console to create stack'
    step1:
      p1: 'You can '
      p2: 'Preview Template'
      p3: ' to understand all the permissions involved.'
    step2:
      heading: 'Review and Lauch Harness specified AWS CloudFormation template on AWS console'
      subStep1: 'Click on the checkbox to acknowledge the IAM role creation.'
      subStep2:
        p1: 'Click on '
        p2: 'Create stack'
    step3:
      p1: 'On stack details page click on '
      p2: 'Outputs'
      p3: 'tab and copy '
      p4: 'CrossAccountRoleArn'
    step4: 'Provide that IAM ARN in Harness UI.'
    step5: 'External Id is a random unique value to provide additional secure authentication.'
  testConnection:
    heading: '{{ $.common.labelTestConnection }}'
    step1: 'Validating AWS Cloud Provider authentication'
    step2: 'Verifying the CUR report and S3 bucket name '
    step3: 'Validating the Cross-account role ARN '
ceAzure:
  guidPlaceholder: 'xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx'
  guidRegexError: 'It has to match GUID pattern. Example: "123e4567-e89b-12d3-a456-9AC7CBDCEE52"'
  steps:
    overview: '{{ $.overview }}'
    billingExports: Azure Billing Exports
    requirements: Choose Requirements
    servicePrincipal: Create Service Principal
    testConnection: '{{ $.common.labelTestConnection }}'
  overview:
    heading: '{{ $.overview }}'
    tenantId: Specify Azure Tenant Id
    subscriptionId: Specify Azure Subscription Id
    alreadyExist: '{{ $.connectors.connectorAlreadyExist }}'
    existingConnectorInfo: The cloud account {{accountId}} already has a Connector “{{name}}” linked to it. The Connector has permissions for {{featureText}}.
    trySuggestion: '{{ $.common.errorHandler.tryTheseSuggestions }}'
    editConnector: '{{ $.connectors.editConnector }}'
    required: '{{ $.connectors.ifReq }}'
  billing:
    heading: '{{ $.connectors.ceAzure.steps.billingExports }}'
    subHeading: Billing export is used to get insights into your cloud infrastructure and Azure services such as Storage account, Virtual machines, Containers etc.
    instruction: Please follow the instructions to provide access to the Billing export for the specified Tenant Id
    login: 'and login to your Azure account'
    launchAzureConsole: Launch Azure Billing Exports
    tooltipInstruction: Provided in the delivery options when the template is opened in the Azure console
    tooltipBtn: '{{ $.connectors.showInstructions }}'
    storageAccountName: Storage Account Name
    storageAccountNameRegexError: It must be 3 to 24 characters long, and can contain only lowercase letters and numbers.
    subscriptionId: Storage Account Subscription Id
    containerName: Storage Container
    directoryName: Storage Directory
    reportName: '{{ $.common.reportName }}'
    extension:
      createBillingExportGuide: How to Create a Billing Export?
      step0: After logging into Azure billing export
      step1: Click on Add to create a new Export.
      step2: In Export Details, provide Name
      step3: For Export type, select "Daily export of month-to-date costs
      step4: Leave the Start date as Today
      step5: For the Storage, select Use existing or Create new.
      step6: If use existing
      step7: Select the Subscription where your storage account is present
      step8: Select the Storage account
      step9: If create new
      step10: Select the Subscription where you want to create the storage account
      step11: Select Resource group name for this storage account
      step12: Provide the Storage account name
      step13: Provide the Location
      step14: Provide the Container name
      step15: Provide the Directory name
      step16: Review your export details and click on Create.
      step17: Your new export appears in the list of exports
      links: 'Useful links:'
      video: Watch help video
      docs: Harness Documentation
      createExport: Creating a Billing Export
      soon: coming soon
  existingExports:
    instruction: 'The following Billing Exports exist in your Harness account. If the Azure Subscription Id you entered is covered by any of the below, then click "Continue" to proceed. If not, click on "Create new Billing Export.'
    createNewExportBtn: '+ Create new Billing Export'
    subscriptionId: Subscription Id
    tenantId: '{{ $.connectors.tenantId }}'
  chooseRequirements:
    heading: Choose Required Permissions
    subHeading: Harness uses Multitenant app to sync billing export data from source storage account to Harness. Create a service principal for this app in your Azure account and assign read permissions on that particular storage account.
    featureDesc: Select the Cloud Cost Management features that you would like to enable.
    info: (Azure cost Visibility is available by default. Select the other option as per your requirements)
    visibilityCardDesc: Cost insights, anomaly detection, service insights, creating budgets perspectives and alerts, utilised/wasted resources in clusters.
    optimizationCardDesc: Detection of orphaned resources, recommendations to save costs, scaling/tearing down, turning off in non-work hours, reserving instances.
    visibility:
      feat1: Azure costs by services, accounts, etc.
      feat2: Cost perspective by various constructs
      feat3: Cost anomaly detection
      feat4: Budgets and forecasts
      feat5: Email and Slack alerts
      footer: from Billing export
    optimization:
      feat1: Orchestrate VMs based on idleness
      feat2: Set dependencies between VMs
      feat3: Granular savings visibility
      feat4: Simple one-time setup
      prefix: AZURE OPTIMIZATION BY
      footer1: by adding
      footer2: to SP
  servicePrincipal:
    heading: '{{ $.connectors.ceAzure.steps.servicePrincipal }}'
    subHeading: Create service principal and assign permissions by running the following commands in bash terminal or in Azure cloud shell
  testConnection:
    heading: '{{ $.common.labelTestConnection }}'
    validatePermission: '{{ $.connectors.testConnectionStep.validationText.azure }}'
    verifyExport: Verifying Azure Billing Export setup
  validation:
    storageAccountName: Storage account name is required
    directoryName: Directory name is required
    containerName: Container name is required
    reportName: '{{ $.common.reportNameRequired }}'
    subscriptionId: Subscription Id is required
    tenantId: '{{ $.connectors.tenantIdRequired }}'
ceGcp:
  overview:
    heading: '{{ $.overview }}'
    projectIdLabel: 'Specify Project Id'
  billingExport:
    heading: 'Setup Billing Export'
    description: 'Cloud Billing export to BigQuery enables you to export detailed Google Cloud billing data (such as usage and cost estimate data) automatically throughout the day to a BigQuery dataset that you specify.'
    followInstruction: 'Please follow the instructions to provide create a Billing export for the specified Project Id'
    launchTemplate: 'Launch GCP console'
    datasetIdLabel: 'Dataset Id'
    tableIdLabel: 'Table Id'
    tooltipDescription: GCP DataSet Id where billing table is available
    tableIdTooltipDesc: GCP Table Id where billing export is available
  billingExtention:
    heading: 'How to create GCP Billing Export and find DataSet Id?'
    prerequisite: 'Pre-Requisite:A project where the Cloud Billing data will be stored with billing enabled on the project.'
    step1: 'Create a BigQuery dataset in which to store the data(Recommended) / Or use Existing DataSet (Harness Docs)'
    step2: '{{ $.connectors.ceGcp.grantPermission.step1}}'
    step3:
      p1: 'Click on Create Dataset, enter a '
      p2: 'DatasetID'
      p3: ' and click on create.'
    step4: 'Enable Cloud Billing export of cost data and pricing data to be written into the dataset.'
    step5: 'Open the console -> Navigation menu (menu), and then select Billing.'
    step6: 'In the Billing navigation menu, select Billing export.'
    step7: 'Select the BigQuery export tab (this tab is selected by default). On the BigQuery export tab, enable Daily cost detail.'
    otherLinks: 'Other Links:'
    link1: 'https://cloud.google.com/billing/docs/how-to/export-data-bigquery-setup'
    link2: 'Harness Docs ()'
  grantPermission:
    heading: 'Grant Permissions'
    bigQueryButtonText: 'Open BigQuery Page'
    step1: 'Sign in to the Google Cloud Console and go to the BigQuery page.'
    step2: 'Click Your-Project-Name in the left panel.'
    step3: 'Select your dataset in your project.'
    step4: 'Click Share Dataset. The Data permissions panel opens.'
    step5: 'Specify Harness’ service account to add member:'
    step6: 'Select BigQuery Data Viewer role.'
    step7: 'Click Continue.'
  testConnection:
    heading: '{{ $.common.labelTestConnection }}'
    step1: 'Verifying GCP connector setup'
    error: 'Could not verify connectivity'
